{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-GRzd7nFZDqn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "n = 20  # context length\n",
        "d = 5  # input dimension\n",
        "L = 3  # number of layers\n",
        "batch_size = 512\n",
        "validation_size = batch_size // 8\n",
        "epochs = 10000\n",
        "\n",
        "# purtabation introduced for initialization\n",
        "delta_initialize = 0.001\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# set seeds\n",
        "\n",
        "# for PyTorch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# to ensure reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "Fu2_iLU4e8iD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution for x and w\n",
        "def generate_data(batch_size, d, n, device):\n",
        "    # x = (torch.rand(batch_size, n+1, d, device=device) - 0.5) * 2 * M  # uniform distribution. of shape (batch_size, n+1, d)\n",
        "    x = torch.randn(batch_size, n+1, d, device=device) # normal distribution N(0, 1). of shape (batch_size, n+1, d)\n",
        "    w_star = torch.randn(batch_size, d, device=device)  # normal distribution N(0, 1) of shape (batch_size, d)\n",
        "    y = (x * w_star.unsqueeze(1)).sum(dim=2) # taking inner product of x_i and w for each batch. of shape(batch_size, n+1)\n",
        "    return x, y, w_star # y[:,n] are the true y_{n+1} values\n",
        "\n",
        "class LinearTransformer(nn.Module):\n",
        "    def __init__(self, d, n, L, delta_init, device):\n",
        "        super(LinearTransformer, self).__init__()\n",
        "        self.L = L\n",
        "        self.n = n\n",
        "        self.device = device\n",
        "\n",
        "        # random\n",
        "        self.P_list = nn.ParameterList([nn.Parameter(torch.randn(d+1, d+1, device=device) * delta_init) for _ in range(L)])\n",
        "        self.Q_list = nn.ParameterList([nn.Parameter(torch.randn(d+1, d+1, device=device) * delta_init) for _ in range(L)])\n",
        "\n",
        "        self.M = torch.block_diag(torch.eye(n, device=device), torch.zeros(1,1, device=device))\n",
        "        # sparse P and Q\n",
        "    #     self.P_list = nn.ParameterList([self.create_P_matrix(d, device) for _ in range(L)])\n",
        "    #     self.Q_list = nn.ParameterList([self.create_Q_matrix(d, device) for _ in range(L)])\n",
        "\n",
        "    # def create_P_matrix(self, d, device):\n",
        "    #     # create a (d+1) x (d+1) matrix with the desired structure for P_i\n",
        "    #     P = torch.zeros(d+1, d+1, device=device)\n",
        "    #     P[-1, -1] = 1\n",
        "    #     return nn.Parameter(P)\n",
        "\n",
        "    # def create_Q_matrix(self, d, device):\n",
        "    #     # create a (d+1) x (d+1) matrix with the desired structure for Q_i\n",
        "    #     Q = torch.zeros(d+1, d+1, device=device)\n",
        "    #     A_i = torch.randn(d, d, device=device) * 0.1  # initialize A_i randomly\n",
        "    #     Q[:d, :d] = A_i  # Place A_i in the top-left block\n",
        "    #     return nn.Parameter(-Q)\n",
        "\n",
        "        # small number\n",
        "        #self.P_list = nn.ParameterList([nn.Parameter(torch.ones(d+1, d+1, device=device) * 0.001) for _ in range(L)])\n",
        "        #self.Q_list = nn.ParameterList([nn.Parameter(torch.ones(d+1, d+1, device=device) * 0.001) for _ in range(L)])\n",
        "\n",
        "        # zero doesn't work\n",
        "        # self.P_list = nn.ParameterList([nn.Parameter(torch.zeros(d+1, d+1, device=device)) for _ in range(L)])\n",
        "        # self.Q_list = nn.ParameterList([nn.Parameter(torch.zeros(d+1, d+1, device=device)) for _ in range(L)])\n",
        "\n",
        "    def forward(self, Z):\n",
        "        # Z is given as shape (batch_size, d+!, n+1)\n",
        "        batch_size = Z.shape[0]\n",
        "        for l in range(self.L):\n",
        "            P = self.P_list[l]\n",
        "            Q = self.Q_list[l]\n",
        "            # batched version of Z_tilde = Z.T @ Q @ Z\n",
        "            Z_tilde = torch.bmm(Z.transpose(1, 2), Q.unsqueeze(0).expand(batch_size, -1, -1))\n",
        "            Z_tilde = torch.bmm(Z_tilde, Z)\n",
        "\n",
        "            # batched version of Attn_PQ = P @ Z @ self.M @ Z_tilde\n",
        "            Attn_PQ = torch.bmm(P.unsqueeze(0).expand(batch_size, -1, -1), Z)\n",
        "            Attn_PQ = torch.bmm(Attn_PQ, self.M.unsqueeze(0).expand(batch_size, -1, -1))\n",
        "            Attn_PQ = torch.bmm(Attn_PQ, Z_tilde)\n",
        "            Z = Z + Attn_PQ / self.n\n",
        "        return Z"
      ],
      "metadata": {
        "id": "6ZhLzxOyfBO4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(ZL, y_true):\n",
        "    y_hat = -ZL[:,-1,-1] # shape (batch_size,)\n",
        "    return torch.mean((y_hat - y_true)**2)"
      ],
      "metadata": {
        "id": "TX9VSu7ZxC32"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for Adam optimizer\n",
        "lr = 0.005\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# number of iteration of experiments\n",
        "num_itr = 11\n",
        "\n",
        "# Function to apply Adam updates with added noise to the gradient\n",
        "def adam_step(params, grads, m, v, t, lr, beta1, beta2, epsilon, noise_scale):\n",
        "    updated_params = []\n",
        "    for idx, (param, grad) in enumerate(zip(params, grads)):\n",
        "        # Add noise to gradient\n",
        "        noisy_grad = grad + torch.randn_like(grad) * noise_scale\n",
        "\n",
        "        # Update biased first moment estimate\n",
        "        m[idx] = beta1 * m[idx] + (1 - beta1) * noisy_grad\n",
        "\n",
        "        # Update biased second moment estimate\n",
        "        v[idx] = beta2 * v[idx] + (1 - beta2) * (noisy_grad ** 2)\n",
        "\n",
        "        # Compute bias-corrected first moment estimate\n",
        "        m_hat = m[idx] / (1 - beta1 ** t)\n",
        "\n",
        "        # Compute bias-corrected second moment estimate\n",
        "        v_hat = v[idx] / (1 - beta2 ** t)\n",
        "\n",
        "        # Update parameters\n",
        "        param_update = param - lr * m_hat / (torch.sqrt(v_hat) + epsilon)\n",
        "        updated_params.append(param_update)\n",
        "\n",
        "    return updated_params, m, v\n",
        "\n",
        "delta_init = 0.01\n",
        "noise_choice = [0.5, 1.0, 2.0]\n",
        "\n",
        "def compute_parameter_distance(params1, params2):\n",
        "    total_distance = 0.0\n",
        "    for p1, p2 in zip(params1, params2):\n",
        "        total_distance += torch.sum((p1 - p2) ** 2).item()\n",
        "    return total_distance\n",
        "\n",
        "def compute_l2_norm(params):\n",
        "    total_norm = 0.0\n",
        "    for param in params:\n",
        "        total_norm += torch.sum(param ** 2).item()\n",
        "    return np.sqrt(total_norm)\n",
        "\n",
        "train_x, train_y, _ = generate_data(batch_size, d, n, device)\n",
        "\n",
        "train_losses_all = []\n",
        "diff_params_all = []\n",
        "\n",
        "for noise_idx in range(len(noise_choice)):\n",
        "    noise = noise_choice[noise_idx]\n",
        "\n",
        "    train_losses = []\n",
        "    diff_params = []\n",
        "    for itr in range(num_itr):\n",
        "        # initialize the model, optimizer, and loss function\n",
        "        model = LinearTransformer(d=d, n=n, L=L, delta_init=delta_init, device=device).to(device)\n",
        "\n",
        "        # Adam's momentum (m) and second moment (v) terms\n",
        "        m = [torch.zeros_like(param) for param in model.parameters()]\n",
        "        v = [torch.zeros_like(param) for param in model.parameters()]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "\n",
        "            # zero-out gradients manually before each training step\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad.zero_()\n",
        "\n",
        "            # forward pass and loss calculation\n",
        "            Z0 = torch.cat((torch.transpose(train_x,1,2), train_y.unsqueeze(1)), dim=1)  # concatenating x and y to form Z\n",
        "            Z0[:,-1,-1] = 0 # initialize y_{n+1} as 0\n",
        "            ZL = model(Z0)  # Apply the linear transformer\n",
        "            train_loss = loss_function(ZL, train_y[:,-1])\n",
        "\n",
        "            # backward pass and manual Adam update with noise\n",
        "            grads = torch.autograd.grad(train_loss, model.parameters(), create_graph=False)\n",
        "\n",
        "            # Set updated parameters back to the model\n",
        "            with torch.no_grad():\n",
        "                # manually perform an Adam step with noisy gradients\n",
        "                updated_params, m, v = adam_step(list(model.parameters()), grads, m, v, epoch+1, lr, beta1, beta2, epsilon, noise_scale=noise)\n",
        "                for param, updated_param in zip(model.parameters(), updated_params):\n",
        "                    param.copy_(updated_param)\n",
        "\n",
        "\n",
        "        train_losses.append(train_loss.item())\n",
        "        with torch.no_grad():\n",
        "            if itr == 0:\n",
        "                prev_param = [param.clone().detach() for param in model.parameters()]\n",
        "            else:\n",
        "                current_param = [param.clone().detach() for param in model.parameters()]\n",
        "                distance = compute_parameter_distance(prev_param, current_param)\n",
        "                norm = compute_l2_norm(prev_param)\n",
        "                # report squared distance and L2-norm of prev_param and ratio of distance and norm to\n",
        "                # grasp an idea of variance\n",
        "                diff_params.append([distance, norm, np.sqrt(distance)/norm])\n",
        "                prev_param = current_param\n",
        "    train_losses_all.append(train_losses)\n",
        "    diff_params_all.append(diff_params)\n",
        "\n",
        "\n",
        "print(train_losses_all)\n",
        "print(diff_params_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5fmMOduWwUk",
        "outputId": "ab1831c2-886a-4447-abfa-226b8d900d14"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.059646882116794586, 0.06962095946073532, 0.08983862400054932, 0.09622515738010406, 0.0709066092967987, 0.0714753121137619, 16573.62890625, 1.4134182929992676, 247264800.0, 0.07128021121025085, 0.06468738615512848], [0.12489436566829681, 0.1903645396232605, 0.12084566801786423, 0.13092964887619019, 3.011082649230957, 155.3625030517578, 0.3229689300060272, 0.12494374811649323, 610.2999267578125, 0.10624580830335617, 0.1545482575893402], [4.3015594482421875, 0.2759762406349182, 1.2465240955352783, 0.20865362882614136, 0.2238343358039856, 0.2649287283420563, 3.8706555366516113, 0.44595056772232056, 0.1916150152683258, 0.25459399819374084, 0.28763097524642944]]\n",
            "[[[30.307927906513214, 7.042401934690144, 0.781730964185434], [80.2342586517334, 6.059773077634685, 1.478167198565795], [98.05196928977966, 6.807984447956252, 1.4544862007144899], [85.66638877987862, 6.701304048342661, 1.3811660893744668], [69.59366434812546, 6.853320201871713, 1.2172613386508673], [63.93245846033096, 6.735994608899585, 1.1870225562359207], [60.26618933677673, 6.261567816578085, 1.2398061240170632], [71.19373309612274, 5.966721174943031, 1.414116336034679], [84.26395386457443, 7.312979837308172, 1.255239336566964], [130.69604432582855, 6.673132248642577, 1.7131740597531306]], [[59.99312138557434, 6.821543456831313, 1.135450168433621], [82.06902998685837, 6.307581628444362, 1.4362391822012701], [31.59307235479355, 6.342156007946516, 0.8862556363822786], [73.83327436447144, 6.337254191038812, 1.355891496965369], [81.56528341770172, 5.572510230909414, 1.620696895782656], [46.00845694541931, 5.998906512753377, 1.1306983021556587], [44.416787415742874, 4.6099525477766115, 1.445696457316252], [79.84344530105591, 6.165871348730656, 1.4491894892206665], [64.57667744159698, 5.932550184083435, 1.3545543255171606], [54.09815174341202, 6.8712576303895965, 1.070421889599875]], [[56.75761556625366, 4.727333472560188, 1.5936605861038315], [52.37534415721893, 6.448663531686986, 1.122260628871805], [70.68937683105469, 4.445650299056055, 1.8912188115158857], [113.00943183898926, 6.463652440509476, 1.644672193888572], [84.45077991485596, 6.068523413359726, 1.5143239829196724], [74.46564435958862, 6.1864605136432225, 1.3948764133348355], [55.98862266540527, 5.811871186975208, 1.2874604949192068], [75.69435024261475, 6.25175968193854, 1.391648184348212], [60.10778295993805, 6.098518266663479, 1.2712794472101958], [103.70280075073242, 6.802702061162665, 1.4969723969358353]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff_05 = list(map(lambda x:x[0], diff_params_all[0]))\n",
        "diff_10 = list(map(lambda x:x[0], diff_params_all[1]))\n",
        "diff_20 = list(map(lambda x:x[0], diff_params_all[2]))\n",
        "print(torch.FloatTensor(diff_05).mean().item(), torch.FloatTensor(diff_10).mean().item(), torch.FloatTensor(diff_20).mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uKOmi4N9GZq",
        "outputId": "7ca5e5c6-d12b-4ea5-f58e-76ee64715396"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77.420654296875 61.79973220825195 74.72417449951172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epmOOOZhSPs7",
        "outputId": "3487e635-3bd3-45e8-d3e9-577f7599f8d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsORq051yOpR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}